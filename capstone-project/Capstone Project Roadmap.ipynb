{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of this project is building an ETL pipeline that extracts Immigration & Airport data from S3, stages them in Redshift, and transforms data into a set of dimensional and fact tables for the analytics team in order to run analytical SQL queries and continue finding insights.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# set visibility\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3d204fc7a909:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>capstone-project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f17f2981cc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lauch spark session\n",
    "APP_NAME = 'capstone-project'\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(APP_NAME)\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(f'Spark is ready\\n')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The purpose of this project is to build Star Schema data model that allows final user query data in efficient way by creating fast-readable and easy to understand tables dependencies with its columns. In order to achive this, ELT pipeline will be built that extracts Immigration & Airport data from S3, stages them in Redshift, and transforms data into a set of dimensional and fact tables for the analytics team to allow analytical team continue finding interesting insights. For table storage and transformation we will be using Redshift Cluster along with Postgres SQL. Moreover, all single step is monitoring and organized thanks to Airflow.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "For the scope of this project data with regards to Immigration, Demographics & Airport was used. Additionally, mapping dictionary from `I94_SAS_Labels_Descriptions.SAS` file was applied to Immigration data. Below is description of mentioned datasets:\n",
    "- **Immigration** - this data comes from the US National Tourism and Trade Office ([link](https://www.trade.gov/national-travel-and-tourism-office))\n",
    "- **Airport codes** - this data contains the list of all airport codes, the attributes are identified in datapackage description. Some of the columns contain attributes identifying airport locations, other codes (IATA, local if exist) that are relevant to identification of an airport ([link](https://datahub.io/core/airport-codes#data))\n",
    "\n",
    "The data downloaded and uploaded into S3 Bucket named `capstone-project-mt` which can be accessible via Launch Cloud Gateway with regards to fifth project (Pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "immigration = spark.read.parquet('sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet').toPandas() # sample file\n",
    "air_codes = pd.read_csv('airport-codes_csv.csv', sep = ',')\n",
    "demo = pd.read_csv('us-cities-demographics.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_quality_check(df: pd.DataFrame, df_name: str) -> None:\n",
    "    null = pd.DataFrame(df.isna().any())\n",
    "    nul_cols = null.loc[null[0] == True].index.tolist()\n",
    "    duplicated_status = df.duplicated().any()\n",
    "    return(\n",
    "        print(f'Table {df_name} contains {df.shape[1]} columns and {df.shape[0]} rows.\\nColumns list with missing values: {nul_cols}.\\nDuplicated status is {duplicated_status}\\n')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table immigration contains 28 columns and 219268 rows.\n",
      "Columns list with missing values: ['i94mode', 'i94addr', 'depdate', 'dtadfile', 'visapost', 'occup', 'entdepd', 'entdepu', 'matflag', 'dtaddto', 'gender', 'insnum', 'airline', 'fltno'].\n",
      "Duplicated status is False\n",
      "\n",
      "Table air_codes contains 12 columns and 55075 rows.\n",
      "Columns list with missing values: ['elevation_ft', 'continent', 'iso_country', 'municipality', 'gps_code', 'iata_code', 'local_code'].\n",
      "Duplicated status is False\n",
      "\n",
      "Table demo contains 12 columns and 2891 rows.\n",
      "Columns list with missing values: ['Male Population', 'Female Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size'].\n",
      "Duplicated status is False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip([immigration, air_codes, demo], ['immigration', 'air_codes', 'demo']):\n",
    "    basic_data_quality_check(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>None</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0  6.0    2016.0  4.0     692.0   692.0   XXX     20573.0 NaN       None     \n",
       "1  7.0    2016.0  4.0     254.0   276.0   ATL     20551.0  1.0      AL       \n",
       "2  15.0   2016.0  4.0     101.0   101.0   WAS     20545.0  1.0      MI       \n",
       "3  16.0   2016.0  4.0     101.0   101.0   NYC     20545.0  1.0      MA       \n",
       "4  17.0   2016.0  4.0     101.0   101.0   NYC     20545.0  1.0      MA       \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0 NaN       37.0    2.0      1.0    None      None     None  T       None     \n",
       "1 NaN       25.0    3.0      1.0    20130811  SEO      None  G       None     \n",
       "2  20691.0  55.0    2.0      1.0    20160401  None     None  T       O        \n",
       "3  20567.0  28.0    2.0      1.0    20160401  None     None  O       O        \n",
       "4  20567.0  4.0     2.0      1.0    20160401  None     None  O       O        \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0  U       None    1979.0   10282016  None   None   None    1.897628e+09   \n",
       "1  Y       None    1991.0   D/S       M      None   None    3.736796e+09   \n",
       "2  None    M       1961.0   09302016  M      None   OS      6.666432e+08   \n",
       "3  None    M       1988.0   09302016  None   None   AA      9.246846e+10   \n",
       "4  None    M       2012.0   09302016  None   None   AA      9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0  None   B2       \n",
       "1  00296  F1       \n",
       "2  93     B2       \n",
       "3  00199  B2       \n",
       "4  00199  B2       "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0  00A   heliport       Total Rf Heliport                   11.0           \n",
       "1  00AA  small_airport  Aero B Ranch Airport                3435.0         \n",
       "2  00AK  small_airport  Lowell Field                        450.0          \n",
       "3  00AL  small_airport  Epps Airpark                        820.0          \n",
       "4  00AR  closed         Newport Hospital & Clinic Heliport  237.0          \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0  NaN       US          US-PA      Bensalem      00A      NaN        \n",
       "1  NaN       US          US-KS      Leoti         00AA     NaN        \n",
       "2  NaN       US          US-AK      Anchor Point  00AK     NaN        \n",
       "3  NaN       US          US-AL      Harvest       00AL     NaN        \n",
       "4  NaN       US          US-AR      Newport       NaN      NaN        \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0  00A        -74.93360137939453, 40.07080078125     \n",
       "1  00AA       -101.473911, 38.704022                 \n",
       "2  00AK       -151.695999146, 59.94919968            \n",
       "3  00AL       -86.77030181884766, 34.86479949951172  \n",
       "4  NaN        -91.254898, 35.6087                    "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0  Silver Spring     Maryland       33.8        40601.0           \n",
       "1  Quincy            Massachusetts  41.0        44129.0           \n",
       "2  Hoover            Alabama        38.5        38040.0           \n",
       "3  Rancho Cucamonga  California     34.5        88127.0           \n",
       "4  Newark            New Jersey     34.6        138040.0          \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0  41862.0            82463             1562.0              30908.0        \n",
       "1  49500.0            93629             4147.0              32935.0        \n",
       "2  46799.0            84839             4819.0              8229.0         \n",
       "3  87105.0            175232            5821.0              33878.0        \n",
       "4  143873.0           281913            5829.0              86253.0        \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0  2.60                    MD         Hispanic or Latino         25924  \n",
       "1  2.39                    MA         White                      58723  \n",
       "2  2.58                    AL         Asian                      4759   \n",
       "3  3.18                    CA         Black or African-American  24437  \n",
       "4  2.73                    NJ         White                      76402  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Based on information above, rows with missing values could be either replaced with zeros or deleted. However, if we go down the road with the last option then there is high probability to omit important information. Competent person from analytics team have to decide which option is the most convenient. We will work further on data as it is.\n",
    "\n",
    "#### Dealing with `I94_SAS_Labels_Descriptions.SAS` file\n",
    "The file contains crucial information with regards to mapping certain columns in Immigration dataset. The mapping fields will be retrived from `I94_SAS_Labels_Descriptions.SAS` into several `.csv` files which will be stored in `helper_tables` folder and upload afterwards into `capstone-project-mt` S3 Bucket. Whole process is mirroring into code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. i94cntyl is processed and saved into helper_tables folder\n",
      "and read again.\n",
      "\n",
      "2. i94prtl is processed and saved into helper_tables folder\n",
      "and read again.\n",
      "\n",
      "3. i94model is processed and saved into helper_tables folder\n",
      "and read again.\n",
      "\n",
      "4. i94addrl is processed and saved into helper_tables folder\n",
      "and read again.\n",
      "\n",
      "5. I94VISA is processed and saved into helper_tables folder\n",
      "and read again.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read SAS file\n",
    "with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "\n",
    "## this function is taken from one of the Mentor's answer from Knowledge serction\n",
    "def code_mapper(file: str, idx: str) -> dict:\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic\n",
    "##\n",
    "\n",
    "# list of keys\n",
    "VAR_CODE_LIST = re.findall('i94.*', f_content) + ['I94VISA']\n",
    "\n",
    "# loop over each key\n",
    "for n,i in enumerate(VAR_CODE_LIST):\n",
    "    try:\n",
    "        (\n",
    "            pd.DataFrame(code_mapper(f_content, i), index = {0})\n",
    "            .T\n",
    "            .reset_index()\n",
    "            .rename(columns = {'index' : 'key', 0 : f\"{i.lower() if i == 'I94VISA' else i}\"})\n",
    "            .to_csv(f\"helper_tables/{i.lower() if i == 'I94VISA' else i}.csv\", index = False)\n",
    "        )\n",
    "        print(f'{n + 1}. {i} is processed and saved into helper_tables folder')\n",
    "        globals()[i.lower() if i == 'I94VISA' else i] = pd.read_csv(f\"helper_tables/{i.lower() if i == 'I94VISA' else i}.csv\")\n",
    "        print('and read again.\\n')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The Star Schema was used in order to create fact table - `immigration` - and dimension tables: \n",
    "- `passenger`\n",
    "- `flights`\n",
    "- `flight_flags`\n",
    "- `visas`\n",
    "- `airport_code`\n",
    "- `country_codes`\n",
    "- `airport_type`\n",
    "- `states`\n",
    "- `airport_city`\n",
    "- `visa_type`\n",
    "\n",
    "One of the reason for using Star Schema is that fact table will joining only with the dimension tables, leading to simpler, faster SQL queries which allows analytical team read data in efficient way. Morevoer, it allows us keep  dimension tables unnormalize while, e.g Snowflake Schemas dimension tables are normalized.\n",
    "\n",
    "<img width=\"1500\" alt=\"db\" src=\"ing/db-schema.png\">\n",
    "\n",
    "#### 3.2 Data dictionary\n",
    "\n",
    "- Immigration table dictionary \n",
    "\n",
    "| immigration table  | Constraint  | Description                                                 |\n",
    "|--------------|-------------|-------------------------------------------------------------|\n",
    "| admission_no | Primary Key | Admission Number                                            |\n",
    "| cic_id       | Foreign Key | Data provided id                                            |\n",
    "| passenger_id | Foreign Key | Passenger id                                                |\n",
    "| flight_id    | Foreign Key | Flight id                                                   |\n",
    "| visa_id      | Foreign Key | Visa id                                                     |\n",
    "| year         |             | Year                                                        |\n",
    "| month        |             | Month                                                       |\n",
    "| day          |             | Day                                                         |\n",
    "| travel_model | Foreign Key | Mode of transportation(1=Air, 2=Sea, 3=Land, 9=Not reported |\n",
    "| count        |             | Summary statistics                                          |\n",
    "\n",
    "- Passenger table dictionary \n",
    "\n",
    "| passenger table        | Constraint  | Description                     |\n",
    "|------------------------|-------------|---------------------------------|\n",
    "| passenger_id           | Primary Key | Passenger id                    |\n",
    "| insnum                 |             | Insurance number                |\n",
    "| gender                 |             | Gender                          |\n",
    "| years                  |             | Ages of passenger               |\n",
    "| birth_year             |             | Year of birth                   |\n",
    "| occupation             |             | Occupation                      |\n",
    "| state_of_residence_abb |             | State of residence abbreviation |\n",
    "| state_of_residence     |             | State of residence              |\n",
    "\n",
    "\n",
    "- Flights table dictionary\n",
    "\n",
    "| flights table          | Constraint  | Description                     |\n",
    "|------------------------|-------------|---------------------------------|\n",
    "| flight_id              | Primary Key | Flight id                       |\n",
    "| flight_no              |             | Flight number                   |\n",
    "| dep_country_id         |             | Departure country id            |\n",
    "| dep_country            |             | Departure country               |\n",
    "| arr_country_id         |             | Arriving country id             |\n",
    "| arr_country            |             | Arriving country                |\n",
    "| airport_city_abb       |             | Airport city abbreviation       |\n",
    "| airport_city_name      |             | Airport city name               |\n",
    "| state_of_residence_abb |             | State of residence abbreviation |\n",
    "| dep_date               |             | Departure data      |\n",
    "|arr_date||Arriving data|\n",
    "\n",
    "- Flight flags table dictionary\n",
    "\n",
    "| flight_flags table | Constraint  | Description                                                              |\n",
    "|--------------------|-------------|--------------------------------------------------------------------------|\n",
    "| cic_id             | Primary Key |  Table id                                                                |\n",
    "| admission_no       |             | Admission number                                                         |\n",
    "| arr_flag           |             | Arrival Flag - admitted or paroled into the U.S.                         |\n",
    "| dep_flag           |             | Departure Flag - Departed, lost I-94 or is deceased                      |\n",
    "| upd_flag           |             | Update Flag - Either apprehended, overstayed, adjusted to perm residence |\n",
    "| match_flag         |             | Match of arrival and departure records                                   |\n",
    "\n",
    "\n",
    "- Visas flags table dictionary\n",
    "\n",
    "| visas table     | Constraint  | Description                                                                        |\n",
    "|-----------------|-------------|------------------------------------------------------------------------------------|\n",
    "| cic_id          | Primary Key | Table id                                                                           |\n",
    "| visa_type_id    |             | Visa codes (1 = Business, 2 = Pleasure, 3 = Student)                               |\n",
    "| visa_type_class |             | Encoded visa codes (1 = Business, 2 = Pleasure, 3 = Student)                       |\n",
    "| visa_type       |             | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |\n",
    "| visa_post       |             | Department of State where where Visa was issued                                    |\n",
    "| admitted_date   |             | Date to which admitted to U.S. (allowed to stay until)                             |\n",
    "\n",
    "- Airport code table dictionary\n",
    "\n",
    "| airport_code table | Constraint  | Description                                                 |\n",
    "|--------------------|-------------|-------------------------------------------------------------|\n",
    "| ident              | Primary Key | Table id                                                    |\n",
    "| type               |             | Airport type (e.g.small, heliport)                          |\n",
    "| name               |             | Airport name                                                |\n",
    "| elevation_ft       |             | Flight number                                               |\n",
    "| continent          |             | Continent                                                   |\n",
    "| iso_country        |             | Country (by International Organization for Standardization) |\n",
    "| iso_region         |             | Region (by International Organization for Standardization)  |\n",
    "| municipality       |             | City                                                        |\n",
    "| gps_code           |             | GPS code                                                    |\n",
    "| iata_code          |             | International Air Transport Association Code                |\n",
    "|local_code||Local code|\n",
    "|coordinates||Such as latitude and longitude|\n",
    "\n",
    "- Airport city table dictionary\n",
    "\n",
    "| airport_city table | Constraint  | Description  |\n",
    "|--------------------|-------------|--------------|\n",
    "| key                | Primary Key | Table id     |\n",
    "| airport_city       |             | Airport city |\n",
    "\n",
    "- Country codes table dictionary\n",
    "\n",
    "| country_codes table | Constraint  | Description |\n",
    "|---------------------|-------------|-------------|\n",
    "| key                 | Primary Key | Table id    |\n",
    "| country             |             | Country     |\n",
    "\n",
    "- Airport type table dictionary\n",
    "\n",
    "| airport_type table | Constraint  | Description  |\n",
    "|--------------------|-------------|--------------|\n",
    "| key                | Primary Key | Table id     |\n",
    "| i94model           |             | Airport type |\n",
    "\n",
    "- states table dictionary\n",
    "\n",
    "| states table       | Constraint  | Description |\n",
    "|--------------------|-------------|-------------|\n",
    "| key                | Primary Key | Table id    |\n",
    "| state_of_residence |             | State       |\n",
    "\n",
    "- visa_type table dictionary\n",
    "\n",
    "| visa_type table | Constraint  | Description                                                  |\n",
    "|-----------------|-------------|--------------------------------------------------------------|\n",
    "| key             | Primary Key | Table id                                                     |\n",
    "| visa_type_class |             | Encoded visa codes (1 = Business, 2 = Pleasure, 3 = Student) |\n",
    "\n",
    "\n",
    "#### 3.3 Mapping Out Data Pipelines\n",
    "The pipeline starts from copying necessary data from S3 Bucket into Redshift Tables. Afterwards, fact table is creating along with dimensional tables. Next steps are two data quality checks that will justify whether:\n",
    "- number of records for all the tables are is expected\n",
    "- primary key for a given table is unique one\n",
    "\n",
    "<img width=\"1500\" alt=\"db\" src=\"ing/data-pipeline.png\">\n",
    "\n",
    "Pipeline logic is shared vid Git Hub Repo ([link](lome-link))\n",
    "\n",
    "Evidence of successful running above pipeline:\n",
    "<img width=\"1500\" alt=\"db\" src=\"ing/airflow-outcome.png\">\n",
    "\n",
    "<img width=\"500\" alt=\"db\" src=\"ing/immigration-count.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Last two steps of presented pipeline are data quality checks that will justify whether:\n",
    "- number of records for all the tables are is expected (simple count of inserted and read rows)\n",
    "- primary key for a given table is unique one (group procedure by primary key with having filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQ_COUNT_DICT = [\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.immigration', 'expected_outcome' : 3096313},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.passenger', 'expected_outcome' : 3096313},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.flights', 'expected_outcome' : 3096313},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.flight_flags', 'expected_outcome' : 3096313},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.visas', 'expected_outcome' : 3096313},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.airport_code', 'expected_outcome' : 55075},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.country_codes', 'expected_outcome' : 289},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.airport_city', 'expected_outcome' : 660},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.airport_type', 'expected_outcome' : 4},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.states', 'expected_outcome' : 55},\n",
    "    {'query' : 'SELECT COUNT(*) FROM public.visa_type', 'expected_outcome' : 3}\n",
    "]\n",
    "\n",
    "DQ_PK_UNIQUE_STATEMENT = '''SELECT {}, COUNT(*) FROM public.{} GROUP BY {} HAVING COUNT(*) > 1'''\n",
    "\n",
    "DQ_PK_UNIQUE_DICT = [\n",
    "    {'query' : DQ_PK_UNIQUE_STATEMENT.format('admission_no', 'immigration', 'admission_no'), 'expected_outcome' : 0},\n",
    "    {'query' : DQ_PK_UNIQUE_STATEMENT.format('passenger_id', 'passenger', 'passenger_id'), 'expected_outcome' : 0},\n",
    "    {'query' : DQ_PK_UNIQUE_STATEMENT.format('flight_id', 'flights', 'flight_id'), 'expected_outcome' : 0},\n",
    "    {'query' : DQ_PK_UNIQUE_STATEMENT.format('cic_id', 'flight_flags', 'cic_id'), 'expected_outcome' : 0},\n",
    "    {'query' : DQ_PK_UNIQUE_STATEMENT.format('visa_id', 'visas', 'visa_id'), 'expected_outcome' : 0},\n",
    "    {'query' : DQ_PK_UNIQUE_STATEMENT.format('ident', 'airport_code', 'ident'), 'expected_outcome' : 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* S3 Bucket was chosen for data storing, it's quite compatible with Redshift Cluster that was utilized as a main engine of table transformation and storage. Whole the process was octhestried by Airflow that was easely connected with S3 and Redshift throught Connections created in Admin tab.\n",
    "* Tables from staging table should be updated either daily, monthly or quarterly depends of data availability and purposes of analytical team. Other data such as `airport_code` or five dictionary tables should remain untouched as their values are not going to change frequently with time.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x - **partition should be introduced or we could use Uber's Hudi incremental inserting tool**\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day - **set up a sensor operator in Airflow that will be monitoring data availability for a particular day and upload all available data into table either with append mode or partition**\n",
    " * The database needed to be accessed by 100+ people - **structural improvement; Redshift is powerful database itself**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
